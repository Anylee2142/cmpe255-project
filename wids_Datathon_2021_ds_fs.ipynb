{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qSwG6SXjb9O4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "url_1 = 'https://raw.githubusercontent.com/takanju/wids_datathon_2021/master/TrainingWiDS2021.csv'\n",
    "train_local_path = './data/TrainingWiDS2021.csv'\n",
    "url_2 = 'https://raw.githubusercontent.com/takanju/wids_datathon_2021/master/UnlabeledWiDS2021.csv'\n",
    "test_local_path = './data/UnlabeledWiDS2021.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2ts3yh6Bov2i"
   },
   "outputs": [],
   "source": [
    "# Splitted data to make same shape of both test and train so that we can apply pre processing on both\n",
    "# Ref : https://www.kaggle.com/siavrez/2020fatures\n",
    "medical_data = pd.read_csv(train_local_path, error_bad_lines=False, index_col=0)\n",
    "test = pd.read_csv(test_local_path, error_bad_lines=False, index_col=0)\n",
    "y = medical_data[\"diabetes_mellitus\"]\n",
    "del medical_data['diabetes_mellitus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znvkr_fsxv-U",
    "outputId": "9f1f5c5b-5e46-4131-b1ba-03c6c79af4c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130157, 179)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "-Uq2mVcAqVML",
    "outputId": "21015ced-3db2-4cd9-ec7a-cd28cb4fed5d"
   },
   "outputs": [],
   "source": [
    "# Percentage of missing values \n",
    "# Is it showing 179 columns?\n",
    "# No.. it gives for 60 as passed in argument ..\n",
    "# pd.DataFrame(medical_data.isna().sum()*100/len(medical_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAlcdHbGEyBW"
   },
   "source": [
    "# Pre processing by Aishwarya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "K1I81SaJE4CR"
   },
   "outputs": [],
   "source": [
    "def preProcessing1(df):\n",
    "    df=df.drop(['encounter_id', 'hospital_id', 'icu_id', 'urineoutput_apache'], axis=1)\n",
    "    #preprocessing for age, height, weight, bmi\n",
    "    df['age'].fillna((df['age'].mean()), inplace=True)\n",
    "    df['bmi'].fillna((df['bmi'].mean()), inplace=True)\n",
    "    df['height'].fillna((df['height'].mean()), inplace=True)\n",
    "    df['weight'].fillna((df['weight'].mean()), inplace=True)\n",
    "\n",
    "    #preprocessing of categorical variable\n",
    "    df['ethnicity'] = df['ethnicity'].fillna(df['ethnicity'].mode().iloc[0])\n",
    "    df['gender'] = df['gender'].fillna(df['gender'].mode( ).iloc[0])\n",
    "    df['hospital_admit_source'] = df['hospital_admit_source'].fillna(df['hospital_admit_source'].mode().iloc[0])\n",
    "    df['icu_admit_source'] = df['icu_admit_source'].fillna(df['icu_admit_source'].mode().iloc[0])\n",
    "    df['icu_stay_type'] = df['icu_stay_type'].fillna(df['icu_stay_type'].mode().iloc[0])\n",
    "    df['icu_type'] = df['icu_type'].fillna(df['icu_type'].mode().iloc[0])\n",
    "\n",
    "    # One Hot Encoding\n",
    "    # Scaler\n",
    "    # apache_2_diagnosis <- impute..., All other numerical features\n",
    "\n",
    "    return df, []\n",
    "\n",
    "# d=preProcessing1(medical_data_reduced.copy())\n",
    "# d\n",
    "\n",
    "# [What kind of numerical columns you used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "\n",
    "def preProcessing1(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df=df.drop(['encounter_id', 'hospital_id', 'icu_id', 'urineoutput_apache'], axis=1)\n",
    "    cols = df.columns\n",
    "    cat_cols = ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "    \n",
    "    num_df = df.loc[:, ~df.columns.isin(cat_cols)]\n",
    "    print(num_df.shape)\n",
    "    num_cols = num_df.columns\n",
    "    cat_df = df.loc[:, cat_cols]\n",
    "    print(cat_df.shape)\n",
    "    \n",
    "    # Simple Imputing\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    imputed_df = imputer.fit_transform(num_df.values)\n",
    "    print('Impute Completed')\n",
    "    print(imputed_df.shape)\n",
    "\n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    imputed_scaled_df = scaler.fit_transform(imputed_df)\n",
    "    print('Standardization Completed')\n",
    "    print(imputed_scaled_df.shape)\n",
    "    \n",
    "    num_df = pd.DataFrame(columns=num_cols, data=imputed_df)\n",
    "    print(num_df.shape)\n",
    "    \n",
    "    index_length = df.shape[0]\n",
    "    num_df.index = range(0, index_length)\n",
    "    cat_df.index = range(0, index_length)\n",
    "    df = pd.concat([num_df, cat_df], axis=1)\n",
    "    print(df.shape)\n",
    "    \n",
    "    return df, [imputer, scaler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhDg_WmLE_-a"
   },
   "source": [
    "# Pre processing by Anjali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WyD1c25LE-jQ"
   },
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy import stats, special\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Ref: https://www.kaggle.com/shashankasubrahmanya/missing-data-imputation-using-regression\n",
    "\n",
    "# filling missing values based on linear regression and the most correlated variables\n",
    "# linear regression -> F-test (which columns are mostly related with the given target column, other than \"diabetes\")\n",
    "# Multi-colinearity\n",
    "# target = diabetes\n",
    "# Remove features which has more than 50% percentage of missing values\n",
    "# Return reduced dataset\n",
    "\n",
    "# def drop_NaN_Values(df, threshold):\n",
    "#     NaN_cols = []\n",
    "#     for col in df.columns:\n",
    "#         NaN_ratio = df[col].isnull().sum() / df.shape[0]\n",
    "#         if NaN_ratio >= threshold:\n",
    "#             NaN_cols.append(col)\n",
    "#     df = df.drop(NaN_cols, axis=1)\n",
    "#     return df\n",
    "\n",
    "# medical_data_reduced = drop_NaN_Values(medical_data.copy(), 0.5)\n",
    "\n",
    "# medical_data_reduced.shape\n",
    "\n",
    "# Doesn't fill 100%. We need to fill so or drop null rows\n",
    "def fillna_using_linear_model(df):\n",
    "    fea_cols=[]\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype=='float64':\n",
    "            fea_cols.append(col)\n",
    "\n",
    "    correl = df[fea_cols].corr()\n",
    "\n",
    "    for col in tqdm(fea_cols):\n",
    "        nan_ratio = df[col].isnull().sum() / df.shape[0]\n",
    "        if nan_ratio > 0:\n",
    "            best_nan_ratio = nan_ratio\n",
    "            best_col = None\n",
    "            for id in correl.loc[(correl[col] > 0.7) | (correl[col] < -0.7), col].index:\n",
    "                nan_temp_ratio = df[id].isnull().sum() / df.shape[0]\n",
    "                if best_nan_ratio > nan_temp_ratio:\n",
    "                    best_nan_ratio = nan_temp_ratio\n",
    "                    best_col = id\n",
    "            if best_col != None:\n",
    "                sub = df[[col, best_col]].copy()\n",
    "                sub = sub.dropna()\n",
    "                reg = LinearRegression(fit_intercept=True).fit(np.expand_dims(sub[best_col], axis=1), sub[col])\n",
    "                #print(reg.score(np.expand_dims(sub[best_col], axis=1), sub[col]))\n",
    "                if reg.score(np.expand_dims(sub[best_col], axis=1), sub[col])>0.7:\n",
    "                    if df.loc[(~df[best_col].isnull()) & (df[col].isnull()), col].shape[0] > 0:\n",
    "                        df.loc[(~df[best_col].isnull()) & (df[col].isnull()), col] = \\\n",
    "                        reg.predict(np.expand_dims(df.loc[(~df[best_col].isnull()) & (df[col].isnull()), best_col], axis=1))\n",
    "\n",
    "    return df, [reg]\n",
    "\n",
    "# def new_features(df):\n",
    "#   df[\"d1_heartrate_max_min_diff\"] = df[\"d1_heartrate_max\"] - df[\"d1_heartrate_min\"]\n",
    "#   return df\n",
    "\n",
    "\n",
    "def preProcessing2(df: pd.DataFrame, y:pd.Series) -> pd.DataFrame:\n",
    "    col=df.columns\n",
    "    df_imp, tf = fillna_using_linear_model(df)\n",
    "  \n",
    "    # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    imputed_scaled_df = scaler.fit_transform(df_imp)\n",
    "    imputed_scaled_df = pd.DataFrame(data=imputed_scaled_df, columns=df.columns)\n",
    "    tf.append(scaler)\n",
    "\n",
    "    # df_imp = new_features(df)\n",
    "    return imputed_scaled_df, tf\n",
    "\n",
    "# dd = preProcessing2(medical_data_reduced.copy())\n",
    "# dd\n",
    "# [numerical columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns = list(df.columns)\n",
    "    \n",
    "  # Simple Imputing\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    imputed_df = imputer.fit_transform(df.values)\n",
    "    print('Impute Completed')\n",
    "\n",
    "  # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    imputed_scaled_df = scaler.fit_transform(imputed_df)\n",
    "    print('Standardization Completed')\n",
    "    \n",
    "    df = pd.DataFrame(columns=columns, data=imputed_df)\n",
    "    \n",
    "    return df, [imputer, scaler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCPgMs4wFGYl"
   },
   "source": [
    "# Pre processing by Jae Woong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzP35KLaFFYP",
    "outputId": "fecb014b-1028-4228-c075-1897c91f9d5b"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer # MICE \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from discretization import MDLP\n",
    "# from mdlp.discretization import MDLP\n",
    "\n",
    "def preProcessing3(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns = list(df.columns)\n",
    "\n",
    "  # Log transform for skewing data\n",
    "  # https://stats.stackexchange.com/questions/267078/why-is-skewed-data-not-preferred-for-modelling\n",
    "  # https://stats.stackexchange.com/questions/299154/the-benefit-of-unskewing-skewed-data\n",
    "    logs_transform_list = ['d1_bilirubin_min', 'd1_bilirubin_max', 'd1_glucose_max', 'h1_bilirubin_max', 'h1_bilirubin_min', 'h1_bun_max', 'h1_bun_min']\n",
    "    df[logs_transform_list] = np.log2(df[logs_transform_list])\n",
    "    print('Log transformation Completed')\n",
    "    \n",
    "  # Simple Imputing\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    imputed_df = imputer.fit_transform(df.values)\n",
    "    print('Impute Completed')\n",
    "\n",
    "  # Standardization\n",
    "    scaler = StandardScaler()\n",
    "    imputed_scaled_df = scaler.fit_transform(imputed_df)\n",
    "    print('Standardization Completed')\n",
    "\n",
    "    df = pd.DataFrame(columns=df.columns, data=imputed_df)\n",
    "\n",
    "    return df, [{\"columns\": logs_transform_list, \"transform\": np.log2}, imputer, scaler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFXSmCyRFRz9"
   },
   "source": [
    "# Pre processing by Uma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "NScHQ9DQFQLN"
   },
   "outputs": [],
   "source": [
    "#ref: https://www.kaggle.com/lhagiimn/7th-place-solution-wids-2021\n",
    "#ref:https://www.kaggle.com/letianyu/wids-2021-notebook\n",
    "#generating new features to improve the score\n",
    "import pandas as pd\n",
    "# OHE for categorical columns - Not performing One Hot Encoding as all features are in binary 0/1.\n",
    "#No numerical features\n",
    "\n",
    "\n",
    "def remove_NaN_Values(df, threshold=0.5):\n",
    "    # store which columns, and drop the same at test set\n",
    "    NaN_cols = []\n",
    "    for col in df.columns:\n",
    "        NaN_ratio = df[col].isnull().sum() / df.shape[0]\n",
    "        if NaN_ratio >= threshold:\n",
    "            NaN_cols.append(col)\n",
    "    df = df.drop(NaN_cols, axis=1)\n",
    "    return df\n",
    "\n",
    "def preProcessing4(df: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "    columns = df.columns\n",
    "\n",
    "    Removed_NaN_df = remove_NaN_Values(df4)\n",
    "\n",
    "    return Removed_NaN_df, [remove_NaN_Values]\n",
    "\n",
    "# df4 = medical_data.iloc[:, 135:]\n",
    "# target = 'diabetes_mellitus'\n",
    "# df4.drop(target, axis=1)\n",
    "# prep_df4, tf4 = preProcessing4(df4, target)\n",
    "\n",
    "# prep_df4, tf4 = prep4(df4)\n",
    "\n",
    "# prep_df = pd.concat([prep_df1, prep_df2, prep_df3, prep_df4], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpfFhyXBF2ld"
   },
   "source": [
    "# Combine all pre processings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6Irnx6nSncti"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130157, 35)\n",
      "(130157, 6)\n",
      "Impute Completed\n",
      "(130157, 35)\n",
      "(130157, 35)\n",
      "(130157, 41)\n"
     ]
    }
   ],
   "source": [
    "# Aishwarya\n",
    "df1 = medical_data.iloc[:, :45]\n",
    "prep_df1, tf1 = preProcessing1(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6Irnx6nSncti"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impute Completed\n"
     ]
    }
   ],
   "source": [
    "# Anjali\n",
    "df2 = medical_data.iloc[:, 45:90]\n",
    "prep_df2, tf2 = preProcessing2(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6Irnx6nSncti"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log transformation Completed\n",
      "Impute Completed\n"
     ]
    }
   ],
   "source": [
    "# Jaewoong\n",
    "df3 = medical_data.iloc[:, 90:135]\n",
    "prep_df3, tf3 = preProcessing3(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6Irnx6nSncti"
   },
   "outputs": [],
   "source": [
    "# Uma\n",
    "df4 = medical_data.iloc[:, 135:]\n",
    "prep_df4, tf4 = preProcessing4(df4, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols += list(prep_df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130157, 41)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130157, 45)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130157, 45)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130157, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [prep_df1, prep_df2, prep_df3, prep_df4]:\n",
    "    each.index = range(0, 130157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130157, 138)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df = pd.concat([prep_df1, prep_df2, prep_df3, prep_df4], axis=1)\n",
    "prep_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95559, 138)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df.dropna(inplace=True)\n",
    "y = y.loc[prep_df.index]\n",
    "prep_df1 = prep_df1.loc[prep_df.index]\n",
    "prep_df2 = prep_df2.loc[prep_df.index]\n",
    "prep_df3 = prep_df3.loc[prep_df.index]\n",
    "prep_df4 = prep_df4.loc[prep_df.index]\n",
    "prep_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95559,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95559, 41)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95559, 7)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(prep_df.index == y.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binning, and then feature selection\n",
    "# or use feature importance from random forest\n",
    "# or dimensionality reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df1_num_columns = set(prep_df1.columns) - set(cat_cols)\n",
    "numerical_columns = list(prep_df1_num_columns) + list(prep_df2.columns) + list(prep_df3.columns)\n",
    "len(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethnicity\n",
      "gender\n",
      "hospital_admit_source\n",
      "icu_admit_source\n",
      "icu_stay_type\n",
      "icu_type\n",
      "aids\n",
      "cirrhosis\n",
      "hepatic_failure\n",
      "immunosuppression\n",
      "leukemia\n",
      "lymphoma\n",
      "solid_tumor_with_metastasis\n",
      "original features =  [('icu_stay_type', 0.0007429549601396547), ('icu_type', 0.0003386539843056739), ('hospital_admit_source', 0.00016717473775491575), ('icu_admit_source', 0.00011462636317424939), ('solid_tumor_with_metastasis', 0.00010795488941162129), ('aids', 7.380556352327899e-05), ('immunosuppression', 4.4247773043782035e-05), ('ethnicity', 3.434455366694478e-05), ('cirrhosis', 3.042511006912982e-05), ('hepatic_failure', 1.9298580778469002e-05), ('leukemia', 1.798427075723074e-05), ('gender', 1.2275541790275845e-05), ('lymphoma', 1.6333667428365178e-07)]\n",
      "\t Fj =  icu_stay_type\n",
      "icu_type\n",
      "\t\t Fi =  icu_type\n",
      "\t\t\t (Redundant) SUij =  0.00538165094703889 SUic 0.0003386539843056739\n",
      "hospital_admit_source\n",
      "\t\t Fi =  hospital_admit_source\n",
      "\t\t\t (Redundant) SUij =  0.00332776046027142 SUic 0.00016717473775491575\n",
      "icu_admit_source\n",
      "\t\t Fi =  icu_admit_source\n",
      "\t\t\t (Redundant) SUij =  0.01272207418438055 SUic 0.00011462636317424939\n",
      "solid_tumor_with_metastasis\n",
      "\t\t Fi =  solid_tumor_with_metastasis\n",
      "aids\n",
      "\t\t Fi =  aids\n",
      "\t\t\t (Redundant) SUij =  0.0002885737592956732 SUic 7.380556352327899e-05\n",
      "immunosuppression\n",
      "\t\t Fi =  immunosuppression\n",
      "\t\t\t (Redundant) SUij =  0.0002506784746738779 SUic 4.4247773043782035e-05\n",
      "ethnicity\n",
      "\t\t Fi =  ethnicity\n",
      "\t\t\t (Redundant) SUij =  0.00165734533279555 SUic 3.434455366694478e-05\n",
      "cirrhosis\n",
      "\t\t Fi =  cirrhosis\n",
      "\t\t\t (Redundant) SUij =  8.837129786647152e-05 SUic 3.042511006912982e-05\n",
      "hepatic_failure\n",
      "\t\t Fi =  hepatic_failure\n",
      "leukemia\n",
      "\t\t Fi =  leukemia\n",
      "\t\t\t (Redundant) SUij =  0.00015226968805869912 SUic 1.798427075723074e-05\n",
      "gender\n",
      "\t\t Fi =  gender\n",
      "\t\t\t (Redundant) SUij =  3.612542080280297e-05 SUic 1.2275541790275845e-05\n",
      "lymphoma\n",
      "\t\t Fi =  lymphoma\n",
      "\t\t\t (Redundant) SUij =  0.0004759941463592165 SUic 1.6333667428365178e-07\n",
      "\t Fj =  solid_tumor_with_metastasis\n",
      "hepatic_failure\n",
      "\t\t Fi =  hepatic_failure\n",
      "\t\t\t (Redundant) SUij =  0.00022697795477858422 SUic 1.9298580778469002e-05\n",
      "removed history for each feature {'icu_stay_type': [{'icu_type': 0.00538165094703889}, {'hospital_admit_source': 0.00332776046027142}, {'icu_admit_source': 0.01272207418438055}, {'aids': 0.0002885737592956732}, {'immunosuppression': 0.0002506784746738779}, {'ethnicity': 0.00165734533279555}, {'cirrhosis': 8.837129786647152e-05}, {'leukemia': 0.00015226968805869912}, {'gender': 3.612542080280297e-05}, {'lymphoma': 0.0004759941463592165}], 'solid_tumor_with_metastasis': [{'hepatic_failure': 0.00022697795477858422}]}\n",
      "best features =  [('icu_stay_type', 0.0007429549601396547), ('solid_tumor_with_metastasis', 0.00010795488941162129)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fcbf import fcbf\n",
    "feature_set, history = fcbf(prep_discretized_df2, y, threshold=0, base=2, is_debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('icu_stay_type', 0.0007429549601396547),\n",
       " ('solid_tumor_with_metastasis', 0.00010795488941162129)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'icu_stay_type': [{'icu_type': 0.00538165094703889},\n",
       "  {'hospital_admit_source': 0.00332776046027142},\n",
       "  {'icu_admit_source': 0.01272207418438055},\n",
       "  {'aids': 0.0002885737592956732},\n",
       "  {'immunosuppression': 0.0002506784746738779},\n",
       "  {'ethnicity': 0.00165734533279555},\n",
       "  {'cirrhosis': 8.837129786647152e-05},\n",
       "  {'leukemia': 0.00015226968805869912},\n",
       "  {'gender': 3.612542080280297e-05},\n",
       "  {'lymphoma': 0.0004759941463592165}],\n",
       " 'solid_tumor_with_metastasis': [{'hepatic_failure': 0.00022697795477858422}]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gE8PgRaJztBB"
   },
   "outputs": [],
   "source": [
    "# Pseudo code\n",
    "# df1 = df.iloc[:, :45]\n",
    "# prep_df1, tf1 = prep1(df1)\n",
    "\n",
    "# df2 = df.iloc[:, 45:90]\n",
    "# prep_df2, tf2 = prep2(df2)\n",
    "\n",
    "# df3 = df.iloc[:, 90:135]\n",
    "# prep_df3, tf3 = prep3(df3)\n",
    "\n",
    "# df4 = df.iloc[:, 135:]\n",
    "# prep_df4, tf4 = prep4(df4)\n",
    "\n",
    "# prep_df = pd.concat([prep_df1, prep_df2, prep_df3, prep_df4], axis=1)\n",
    "\n",
    "\n",
    "# test_df1 = test_df.iloc[:, :45]\n",
    "# prep_test_df1 = test_prep1(test_df1, tf1)\n",
    "\n",
    "# test_df2 = test_df.iloc[:, 45:90]\n",
    "# prep_test_df2 = test_prep2(test_df2, tf2)\n",
    "\n",
    "# test_df3 = test_df.iloc[:, 90:135]\n",
    "# prep_test_df3 = test_prep3(test_df3, tf3)\n",
    "\n",
    "# test_df4 = test_df.iloc[:, 135:]\n",
    "# prep_test_df4 = test_prep4(test_df4, tf4)\n",
    "\n",
    "# prep_test_df = pd.concat([prep_test_df1, prep_test_df2, prep_test_df3, prep_test_df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGSbjpPR5Qah"
   },
   "outputs": [],
   "source": [
    "df1 = df.iloc[:, :45]\n",
    "def train_prep1(df: pd.DataFrame) -> (pd.DataFrame, list):\n",
    "  # Examples\n",
    "  transformer1 = SimpleImputer()\n",
    "  df = transformer1.fit_transform(df)\n",
    "  transformer2 = StandardScaler()\n",
    "  df = transformer2.fit_transform(df)\n",
    "  # MDLP\n",
    "  # LinearRegression.fit_transform\n",
    "  # ...\n",
    "\n",
    "  return df, [transformer1, transformer2]\n",
    "prep_df1, tf1 = train_prep1(df1)\n",
    "\n",
    "# Do the same for 2~4...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJdEZAqc40BH"
   },
   "outputs": [],
   "source": [
    "prep_df = pd.concat([prep_df1, prep_df2, prep_df3, prep_df4], axis=1)\n",
    "\n",
    "\n",
    "test_df1 = test.iloc[:, :45]\n",
    "def test_prep1(df: pd.DataFrame, tf: list) -> pd.DataFrame:\n",
    "  for each in tf:\n",
    "    df = each.transform(df)\n",
    "\n",
    "  return df\n",
    "prep_test_df1 = test_prep1(test_df1, tf1)\n",
    "# Do the same for 2~4...\n",
    "prep_test_df = pd.concat([prep_test_df1, prep_test_df2, prep_test_df3, prep_test_df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_OjdmJCztMf"
   },
   "outputs": [],
   "source": [
    "# For test data\n",
    "# Before imputing make sure size of train and test data should be same so delete target from train and keep train's target as y\n",
    "# I did same on the top (in commented since Jae woong used target value in its pre processing. @Jaewoong: pls make some changes with target name in your function )\n",
    "\n",
    "test1 = preProcessing1(test.copy())\n",
    "test2 = preProcessing2(test1.copy())\n",
    "test3 = preProcessing3(test2.copy())\n",
    "#test4 = preProcessing4(test3.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8adaUmywzPL"
   },
   "outputs": [],
   "source": [
    "target = 'diabetes_mellitus'\n",
    "medical_data[target].value_counts().plot(kind=\"pie\", explode=[0,0.1], autopct=\"%.2f\", labels=[\"No\",\"Yes\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EP1YrRVjxA31"
   },
   "outputs": [],
   "source": [
    "#The dataset is skewed towards class 0, so,balace the dataset.\n",
    "df_majority = medical_data[medical_data['diabetes_mellitus']==0]\n",
    "df_minority = medical_data[medical_data['diabetes_mellitus']==1]\n",
    "\n",
    "# Upsampling\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "                                 replace=True,       # sample with replacement\n",
    "                                 n_samples=83798,    # to match majority class\n",
    "                                 random_state= 303)  # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.diabetes_mellitus.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkLx3oYTBd4I"
   },
   "outputs": [],
   "source": [
    "# [Jaewoong]\n",
    "# extract numberical columns\n",
    "# discretize\n",
    "# feature selection\n",
    "# -> optimal subset of features\n",
    "\n",
    "\n",
    "# [Uma]-Done\n",
    "# Oversampling or Undersampling (=Resampling)\n",
    "# negative class = 75%\n",
    "# positive class = 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWeBNdNqFXts"
   },
   "outputs": [],
   "source": [
    "# [Aishwarya, Anjali]\n",
    "# Training our models\n",
    "# (algorithms(+hparams), CV, gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJPLOFKeB50Q"
   },
   "outputs": [],
   "source": [
    "# Preprocess on test-set -> Evaluate\n",
    "# scaler = scaler()\n",
    "# prep_train = scaler.fit_transform(train)\n",
    "\n",
    "# prep_test = scaler.transform(test)\n",
    "# prep_test = scaler.fit_transform(test) # X -> Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBffQvGxDWgq"
   },
   "outputs": [],
   "source": [
    "# Clustering\n",
    "# WSS, BSS, Entropy, Purity"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wids_Datathon_2021.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cmpe255-project",
   "language": "python",
   "name": "cmpe255-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
